---
layout: homepage
---

*(My last name is pronounced /upadʰje/ but /upaðje/ or oo-pa-the-ye is a close approximation)*

I'm a fifth-year PhD student in the Department of Language Science at the University of California - Irvine, where I'm advised [Dr. Richard Futrell](https://www.socsci.uci.edu/~rfutrell/) and affiliated with the [Language Processing Group](https://langprocgroup.github.io/). Broadly speaking, I'm interested in understanding the representations and mechanisms underlying language use in humans and (large) language models (LLMs). Presently, I'm fascinated by several questions at the heart of language production — a generative process that is somehow, often pardoxically, described as both *easy* and *effortful*. Much of my ongoing work — which draws on computational cognitive modeling, corpus analysis, experimental and NLP methods — focuses on how information processing constraints and communicative pressures shape how humans generate language.

Before coming to UC Irvine, I earned my Bachelor's degree in Cognitive Science with a concentration in Machine Learning and Neural Computation and a minor in Math from the far far away land of UC San Diego. During undergrad, I was a computational neurosciences intern at the National Center for Microscopy and Imaging Research (NCMIR), and my senior research focused on investigating the interaction between lexico-semantic and discourse structure knowledge in LMs. Understanding the representational and mechanistic properties of LMs is still an active research interest of mine, and some of my ongoing projects in this area use psycholinguistically motivated approaches to explore pragmatic reasoning and mutual intelligibility in LLMs.

### Updates
**[Nov 2025]** I will present our poster on trade-offs in advance and incremental planning in spoken and typed sentence production at [Psychonomics](https://www.psychonomic.org/general/custom.asp?page=2025annualmeeting) in Denver, CO. 

**[Oct- Nov 2025]** Our work on human-LM alignment in the interpretation and production of the scopally ambiguous every-negation construction (e.g., *every vote doesn't count*) will feature at both the [PragLM](https://sites.google.com/berkeley.edu/praglm/) and [BlackBoxNLP](https://blackboxnlp.github.io/2025/) workshops at COLM and EMNLP, respectively.
